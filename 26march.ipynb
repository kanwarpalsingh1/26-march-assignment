{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edab4e10-9e4e-4337-bd34-247b88e84a0a",
   "metadata": {},
   "source": [
    "\n",
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "Simple Linear Regression: Simple linear regression involves predicting a dependent variable (target) based on a single independent variable (feature). The relationship between the two variables is assumed to be linear. For example, predicting house prices based on the area of the house.\n",
    "\n",
    "Multiple Linear Regression: Multiple linear regression extends simple linear regression to predict the dependent variable using multiple independent variables. Each independent variable has its coefficient, representing the strength and direction of its relationship with the dependent variable. For example, predicting a person's salary based on their education level, years of experience, and age.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32837c-6926-411e-950c-f3bea863d60b",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "The assumptions of linear regression include:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables is linear.\n",
    "Independence: The residuals (errors) are independent of each other.\n",
    "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables.\n",
    "Normality: The residuals follow a normal distribution.\n",
    "You can check these assumptions by:\n",
    "\n",
    "Visual inspection of scatterplots for linearity.\n",
    "Plotting residuals against fitted values for independence and homoscedasticity.\n",
    "Using statistical tests like the Shapiro-Wilk test for normality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388b702-a996-4871-805c-7e2930018fad",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "Slope (Coefficient): It represents the change in the dependent variable for a one-unit change in the independent variable, holding other variables constant.\n",
    "Intercept: It represents the value of the dependent variable when all independent variables are zero.\n",
    "Example: In a simple linear regression predicting exam scores (dependent variable) based on study hours (independent variable), the slope represents the change in exam score for each additional hour studied, and the intercept represents the expected exam score when the student studied zero hours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ca3ba-7074-43fc-8566-2b7428cde428",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models. It works by iteratively updating the parameters (coefficients) of the model in the direction of the steepest descent of the loss function gradient. This process continues until convergence, where the gradient approaches zero, indicating that further updates will not significantly decrease the loss.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb1e67-75cb-41d8-a57f-70f418f98d24",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "Multiple linear regression predicts the dependent variable using multiple independent variables. It differs from simple linear regression in that it incorporates more than one predictor variable, allowing for a more complex modeling of the relationship between the predictors and the dependent variable. Each independent variable has its coefficient in the model, representing its unique contribution to the prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dec0f-34df-40b9-bbe3-c20d6da17125",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "Multicollinearity occurs when independent variables in a multiple linear regression model are highly correlated with each other. This can cause issues with coefficient estimates and interpretation of the model. Multicollinearity can be detected using correlation matrices or variance inflation factors (VIFs). To address multicollinearity, you can:\n",
    "\n",
    "Remove one of the correlated variables.\n",
    "Combine correlated variables into a single composite variable.\n",
    "Use regularization techniques like ridge regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81826f-76b0-4de1-ae26-3ad74f34e5a5",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Polynomial regression is a form of regression analysis where the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial. Unlike linear regression, which assumes a linear relationship between the variables, polynomial regression can capture non-linear relationships between the variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2034695-ed12-4e76-adcc-5a37e4c637f4",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Advantages of polynomial regression:\n",
    "\n",
    "Can model non-linear relationships between variables.\n",
    "Provides more flexibility in modeling complex relationships.\n",
    "Can fit data more accurately than linear regression in some cases.\n",
    "Disadvantages of polynomial regression:\n",
    "\n",
    "Can lead to overfitting, especially with high-degree polynomials.\n",
    "Interpretation of coefficients becomes more complex with higher-degree polynomials.\n",
    "Requires careful consideration of model complexity and validation techniques.\n",
    "Polynomial regression is preferred when the relationship between the variables is non-linear and cannot be adequately captured by linear regression. It is commonly used in fields like engineering, physics, and economics, where non-linear relationships are prevalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f9eef-19ba-40e3-b0f5-a8dde83bb4de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
